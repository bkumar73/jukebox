{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c23b0470-4411-4e38-aa12-4d8e94bce91e",
   "metadata": {},
   "source": [
    "## üêà Building the model\n",
    "\n",
    "In the journey of building a machine learning model, one of the first decisions is choosing the right type of model‚Äîpredictive or generative. Predictive models focus on forecasting outcomes based on input data, while generative models aim to learn the underlying distribution of data to generate new samples.\n",
    "\n",
    "Our usecase is categorized under predictive machine learning. There are many different ways to build a predictive model. For our use case, we chose neural networks. It has the ability to generalize better, handle complex patterns, and more expressive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c43f41-d612-4ef4-ba0d-7123dee92738",
   "metadata": {},
   "source": [
    "## üê† Install & Import packages\n",
    "\n",
    "Again, we will need to install and import packages as we develop our notebook.\n",
    "\n",
    "This will take a couple of minutes, and if `pip` gives an Error, don't worry about it. Things will just run fine regardless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11590888",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install keras \"tensorflow==2.15.1\" \"tf2onnx\" \"onnx\" \"seaborn\" \"onnxruntime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a801f7dd-604c-4b45-82ad-2ee572ed9372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import os\n",
    "import logging, warnings\n",
    "import random\n",
    "\n",
    "# Suppress warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation, Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import tf2onnx\n",
    "import onnx\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import onnxruntime as rt\n",
    "\n",
    "#Set some seeds\n",
    "SEED = 42\n",
    "# np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "# random.seed(SEED)\n",
    "# os.environ['PYTHONHASHSEED'] = str(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d92117d-f726-4c71-b9fb-fef61bd957ff",
   "metadata": {},
   "source": [
    "# üì¶ Load Data\n",
    "\n",
    "We again load our two datasets, merge them, drop the NA columns just like before and select the input and output data.\n",
    "\n",
    "Input data (X) is the feature matrix that contains the characteristics of each song.\n",
    "\n",
    "Output data (y)is the target variable the model is trying to predict. In this case, y is the 'country' column which represents the country where the song is popular. The model will learn to predict the country based on the song features in X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cc19be-13b7-4063-ae74-08f2400e492f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "song_properties = pd.read_parquet('https://github.com/rhoai-mlops/jukebox/raw/refs/heads/main/99-data_prep/song_properties.parquet')\n",
    "song_rankings = pd.read_parquet('https://github.com/rhoai-mlops/jukebox/raw/refs/heads/main/99-data_prep/song_rankings.parquet')\n",
    "song_properties.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b38b8a-07f3-4c08-9d48-026e843013f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove missing values (NaNs) from the dataset\n",
    "song_rankings = song_rankings.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a70d26c",
   "metadata": {},
   "source": [
    "Before training the model, we need to prepare the data properly:  \n",
    "\n",
    "1. Encoding the Target Variable (`y`):  Our target variable (`y`) is the **country** where a song is popular. However, machine learning models work with numbers, not text labels. We use a Label Encoder to convert country names into numerical values.  \n",
    "\n",
    "2. Splitting the Data: We split the dataset into training, validation, and test sets to ensure the model generalizes well.  \n",
    "   - Training set (`X_train, y_train`): Used to train the model.  \n",
    "   - Validation set (`X_val, y_val`): Helps fine-tune the model and avoid overfitting.  \n",
    "   - Test set (`X_test, y_test`): Used at the very end to evaluate how well the model performs on unseen data.  \n",
    "\n",
    "1. Feature Scaling: Since our input features (e.g., `duration_ms`, `danceability`, `loudness`) have different ranges, we scale them between 0 and 1 using `MinMaxScaler`. This prevents large-valued features from dominating smaller ones and helps the model learn efficiently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851fc269-bd9b-4303-b79b-1554e5c66a60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X is the input fetures we want to train the model on while y is the output feature we want the model to predict.\n",
    "X = song_rankings.merge(song_properties, on='spotify_id', how='left')\n",
    "X = X[['is_explicit', 'duration_ms', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']]\n",
    "y = song_rankings['country']\n",
    "\n",
    "# We use a label encoder to get numbers instead of country codes\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_one_hot = tf.keras.utils.to_categorical(y_encoded)\n",
    "\n",
    "# Split the data into training and testing sets so you have something to test the trained model with.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size = 0.2, shuffle = False, random_state=SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size = 0.2, stratify = y_train, random_state=SEED)\n",
    "\n",
    "# Scale the data to remove mean and have unit variance. The data will be between -1 and 1, which makes it a lot easier \n",
    "# for the model to learn than random (and potentially large) values.\n",
    "# It is important to only fit the scaler to the training data, otherwise you are leaking information about the global \n",
    "# distribution of variables (which is influenced by the test set) into the training set.\n",
    "scaler = MinMaxScaler()\n",
    "scaled_x_train = pd.DataFrame(scaler.fit_transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "scaled_x_val = pd.DataFrame(scaler.transform(X_val), index=X_val.index, columns=X_val.columns)\n",
    "scaled_x_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67168c8-2dd1-4557-a6c3-13f6a4ed63dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# üõü Prepare to save the model\n",
    "\n",
    "Before we start building neural network and training the model, let's prepare the environment to store the resulting artifacts. \n",
    "\n",
    "We need to store our model artifacts in an S3 buckets with folder called models/model-name/version/ for versioning reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c1a425-8c72-4abb-ae81-fb86971e0718",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create local directories to save the model artifacts before starting building neural network and training the model\n",
    "Path(\"models/jukebox/1/artifacts\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(\"models/jukebox/1/artifacts/scaler.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(scaler, handle)\n",
    "\n",
    "with open(\"models/jukebox/1/artifacts/label_encoder.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(label_encoder, handle)\n",
    "\n",
    "with open(\"models/jukebox/1/artifacts/y_test.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(y_test, handle)\n",
    "\n",
    "X_train.to_parquet(\"models/jukebox/1/artifacts/X_train.parquet\")\n",
    "X_test.to_parquet(\"models/jukebox/1/artifacts/X_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49579786-b17a-406b-a9ff-069f822c9fa1",
   "metadata": {},
   "source": [
    "# üöÄ Build the model\n",
    "\n",
    "The below piece of code is like creating a smart helper, aka Model, that learns to guess which countries might like a song based on its characteristics (its features). To process these features, our model will pass them through multiple layers of \"neurons\". These work much like in our brain and the more layers and neurons it has, the more capacity it has for learning.\n",
    "\n",
    "At the end, our model uses what it learned to predict the countries that would enjoy the song the most. \n",
    "\n",
    "Finally, we check how well our model is doing at making these guesses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c988d274-a415-49a3-99a1-caa2b610d7d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Have a dense layer for each individual input?\n",
    "inputs = [Input(shape=(1,), name=name) for name in X.columns]\n",
    "concatenated_inputs = Concatenate(name=\"input\")(inputs)\n",
    "x = Dense(32, activation='relu', name=\"dense_0\")(concatenated_inputs)\n",
    "x = Dense(64, name=\"dense_1\")(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(128, name=\"dense_2\")(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(256, name=\"dense_3\")(x)\n",
    "x = Activation('relu')(x)\n",
    "output = Dense(y_one_hot.shape[1], activation='softmax', name=\"dense_4\")(x)\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825d237b-e9b5-4863-871a-0b11d82a95d1",
   "metadata": {},
   "source": [
    "# ü™ø Model Summary\n",
    "Now let's run `model.summary()` which prints out the blueprint of your music recommendation helper. \n",
    "\n",
    "When you run it, you'll see a long table that shows all the different processing stages your song features go through before turning into country predictions (please don't be scared of the output lenght!).\n",
    "\n",
    "The table shows each layer of your music brain üß†, starting with how it receives the song characteristics, then how it processes them through various \"thinking layers\" (those `Dense` and `Activation` parts), and finally how it produces its country predictions.\n",
    "\n",
    "For each layer, you'll see its name, the shape of information flowing through it (like how many numbers are being processed), and how many \"learning knobs\" (`parameters`) that layer has to adjust as it gets better at predictions ü§ì\n",
    "\n",
    "At the bottom, you'll see the total number of these learning knobs which tells you how complex your recommendation system is. A higher number means your system can potentially learn more complex patterns in music preferences across different countries. (that's why you keep hearing number of parameters a lot in LLM conversations as well)\n",
    "\n",
    "This summary can help you understand the complexity of your music recommendation system without needing to understand all the math happening behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ae9dba-2c70-4652-b032-f77f0d897908",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daf4f12-9802-4bfa-bb79-4043550604a3",
   "metadata": {},
   "source": [
    "# üèÉ Train the Model\n",
    "\n",
    "Now we train our smart helper to predict which country might like a song based on its features. We set it to learn from the training data for 2 epochs, which means that it sees the full dataset two times. During each round, it looks at the song characteristics (scaled_x_train) and the country labels (y_train). It also predicts on a separate dataset called the validation dataset (X_val and y_val) after each epoch. This is to see how well it does on data it hasn't trained on yet.\n",
    "Remember we split the data into three in an above cell. That was the reason :)\n",
    "\n",
    "Once the training is finished, we print a message to let us know that our model is ready to make predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701d17cc-f746-4d3f-8411-5203e3378ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(X_train.columns)\n",
    "\n",
    "train_features = [scaled_x_train[[name]].to_numpy() for name in feature_names]\n",
    "val_features = [scaled_x_val[[name]].to_numpy() for name in feature_names]\n",
    "\n",
    "train_feature_dataset = tf.data.Dataset.zip(tuple(\n",
    "    tf.data.Dataset.from_tensor_slices(f) for f in train_features\n",
    "))\n",
    "val_feature_dataset = tf.data.Dataset.zip(tuple(\n",
    "    tf.data.Dataset.from_tensor_slices(f) for f in val_features\n",
    "))\n",
    "\n",
    "train_dataset = tf.data.Dataset.zip((train_feature_dataset, tf.data.Dataset.from_tensor_slices(y_train)))\n",
    "val_dataset = tf.data.Dataset.zip((val_feature_dataset, tf.data.Dataset.from_tensor_slices(y_val)))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(y_train), seed=42, reshuffle_each_iteration=False)\n",
    "train_dataset = train_dataset.batch(32)\n",
    "val_dataset = val_dataset.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af235a95-9fff-41e7-a799-4831f44c18f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset, \n",
    "    epochs=2, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6ed20d-3f7c-45a2-827a-cd8d36acd967",
   "metadata": {},
   "source": [
    "# ü´° Save the Model\n",
    "\n",
    "Here we convert our trained song prediction model into a popular format called ONNX.  \n",
    "We also save the original Keras model for use later on so we easier can scan it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c36515-4bf9-4017-bfb9-71ab3d8966e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_signature = [tf.TensorSpec(i.shape, i.dtype, i.name) for i in model.inputs]\n",
    "model.output_names = ['output']\n",
    "onnx_model_proto, _ = tf2onnx.convert.from_keras(model, input_signature)\n",
    "onnx.save(onnx_model_proto, \"models/jukebox/1/model.onnx\")\n",
    "\n",
    "model.save('models/jukebox/1/model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b8f29-b610-414d-b1e3-e76d3ea38093",
   "metadata": {},
   "source": [
    "### Quiz Time ü§ì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a0e505-1bfa-47b9-9570-0ee47b5fe8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../.dontlookhere/'))\n",
    "try: from quiz2 import *\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c03ff3f-8ee1-447c-a89f-8ec47000f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: quiz_model() \n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72254830-e3df-46ea-93f0-8c63065a2a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: quiz_nn()\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3265f960-a706-44fb-bd8c-3b6dd1d7e336",
   "metadata": {},
   "source": [
    "# üî• Load the Model for Testing\n",
    "\n",
    "Here we load the model to predict which country might like a song. We open a session to the model and feed in the test data (X_test). The model outputs predictions, and identifies the countries.\n",
    "\n",
    "The accuracy is calculated by comparing the predicted countries to the actual ones as we have the actual answers in our data. That's how we get the accuracy metrics.\n",
    "\n",
    "We also create a confusion matrix to visualize the prediction results, using a heatmap to show how well the model's predictions match the actual labels. We want the predicted country to be the same as the actual country as much as possible, which is visualized by having dark squares on the diagonal, so that for example country 0 often is predicted as country 0 (not as any other country).\n",
    "In other words, the darker the diagonal line, the closer we get to good predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4355b2ea-0daf-47c9-acdf-4b1e8f534ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = {name: scaled_x_test[[name]].to_numpy() for name in X_test.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576e0946-0e36-42ac-821f-284949f1548c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = rt.InferenceSession(\"models/jukebox/1/model.onnx\", providers=rt.get_available_providers())\n",
    "output_name = sess.get_outputs()[0].name\n",
    "y_pred_temp = sess.run([output_name], test_inputs)\n",
    "y_pred_temp = y_pred_temp[0]\n",
    "y_pred_argmax = np.argmax(y_pred_temp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a251400f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_test_argmax = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e216592",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy = np.sum(y_pred_argmax == y_test_argmax) / len(y_pred_argmax)\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "\n",
    "c_matrix = confusion_matrix(y_test_argmax,y_pred_argmax)\n",
    "ax = sns.heatmap(c_matrix, cmap='Blues')\n",
    "ax.set_xlabel(\"Prediction\")\n",
    "ax.set_ylabel(\"Actual\")\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87250c39-6c44-453a-bd27-91408f42a1c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "And now we need to save the model in our S3 bucket to make it available outside of this Notebook. So please open up the [2-save_model.ipynb](2-save_model.ipynb) :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
